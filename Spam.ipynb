{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Email Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this proect I will create a binary classifier that can distinguish spam emails from ham (non-spam) emails using real emails as my dataset. The dataset is from [SpamAssassin](https://spamassassin.apache.org/old/publiccorpus/). It consists of email messages, email IDs, subject lines and their labels (0 for ham, 1 for spam). My training dataset will have 8,348 labeled examples, and the unlabeled test set contains 1,000 unlabeled examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(style = \"whitegrid\", \n",
    "        color_codes = True,\n",
    "        font_scale = 1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile('spam_ham_data.zip') as item:\n",
    "    item.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading training and test datasets\n",
    "original_training_data = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "# Convert the emails to lowercase as the first step of text processing.\n",
    "original_training_data['email'] = original_training_data['email'].str.lower()\n",
    "test['email'] = test['email'].str.lower()\n",
    "\n",
    "original_training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filling in Nan values\n",
    "print('Before imputation:')\n",
    "print(original_training_data.isnull().sum())\n",
    "original_training_data = original_training_data.fillna('')\n",
    "print('------------')\n",
    "print('After imputation:')\n",
    "print(original_training_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Examples of a spam and ham email\n",
    "first_ham = original_training_data.loc[original_training_data['spam'] == 0, 'email'].iloc[0]\n",
    "first_spam = original_training_data.loc[original_training_data['spam'] == 1, 'email'].iloc[0]\n",
    "print(first_ham)\n",
    "print(first_spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The spam email is an ad, it is trying to convince the reader that they should check out their product. This could be marked as spam by looking at key phrases such as \"come in here and see how...\", and \"our methods are guaranteed...\" or similar phrases. The ham email includes a personal phrase \"thanks, misha!\" which makes it seem less automated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates a 90/10 train-validation split on our labeled data.\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, val = train_test_split(original_training_data, test_size = 0.1, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering and Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I create a function called `words_in_texts` that takes in a list of `words` and a pandas `Series` of email `texts`. It outputs a 2-dimensional `NumPy` array containing one row for each email text. The row contains 0 or 1 values associate with each word in the `words` list. If j-th word in the `words` exists in the i-th input of the email `texts` Series, the output element at index (i, j) will be 1, otherwise it will be 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_in_texts(words, texts):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        words (list): words to find\n",
    "        texts (Series): strings to search in\n",
    "    \n",
    "    Returns:\n",
    "        A 2D NumPy array of 0s and 1s with shape (n, p) where \n",
    "        n is the number of texts and p is the number of words.\n",
    "    \"\"\"\n",
    "    indicator_array = []\n",
    "    for i in texts:\n",
    "        input = []\n",
    "        for j in words:\n",
    "            if j in i:\n",
    "                input.append(1)\n",
    "            else:\n",
    "                input.append(0)\n",
    "        indicator_array.append(input)\n",
    "    return indicator_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to identify some features that allow us to distinguish spam emails from ham emails. One idea is to compare the distribution of a single feature in spam emails to the distribution of the same feature in ham emails. If the feature is itself a binary indicator, such as whether a certain word occurs in the text, this amounts to comparing the proportion of spam emails with the word to the proportion of ham emails with the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "df = pd.DataFrame({\n",
    "    'word_1': [1, 0, 1, 0],\n",
    "    'word_2': [0, 1, 0, 1],\n",
    "    'type': ['spam', 'ham', 'ham', 'ham']\n",
    "})\n",
    "display(Markdown(\"> Our Original DataFrame has a `type` column and some columns corresponding to words. You can think of each row as a sentence, and the value of 1 or 0 indicates the number of occurences of the word in this sentence.\"))\n",
    "display(df);\n",
    "display(Markdown(\"> `melt` will turn columns into entries in a variable column. Notice how `word_1` and `word_2` become entries in `variable`; their values are stored in the value column.\"))\n",
    "display(df.melt(\"type\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following plot (which was created using `sns.barplot`) compares the proportion of emails in each class containing a particular set of words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_copy = train.copy()\n",
    "words = ['discount', 'deal', 'guarantee', 'urgent', 'fast', 'regret']\n",
    "#words = ['body', 'business', 'html', 'money', 'offer', 'please']\n",
    "for word in words:\n",
    "    results = []\n",
    "    for email in train['email']:\n",
    "        if (word in email):\n",
    "            results.append(1)\n",
    "        else:\n",
    "            results.append(0)\n",
    "    train_copy[word] = results\n",
    "spams = {}\n",
    "hams = {}\n",
    "for i in words:\n",
    "    spam = sum(train_copy[(train_copy[i] == 1) & (train_copy['spam'] == 1)][i])\n",
    "    ham = sum(train_copy[(train_copy[i] == 1) & (train_copy['spam'] == 0)][i])\n",
    "    spam_prop = spam/sum(train_copy['spam'])\n",
    "    ham_prop = ham/(len(train_copy[i]) - sum(train_copy['spam']))\n",
    "    spams[i] = spam_prop\n",
    "    hams[i] = ham_prop\n",
    "df = pd.DataFrame()\n",
    "for key in spams:\n",
    "    df[key] = [hams[key], spams[key]]\n",
    "df['type'] = ['Ham', 'Spam']\n",
    "df2 = df.melt('type')\n",
    "\n",
    "train = train.reset_index(drop=True) # We must do this in order to preserve the ordering of emails to labels for words_in_texts\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "sns.barplot(data = df2, x = 'variable', y = 'value', hue = 'type')\n",
    "plt.title(\"Frequency of Words in Spam/Ham Emails\")\n",
    "plt.ylabel(\"Proportion of Emails\")\n",
    "plt.xlabel(\"Words\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_words = ['drug', 'bank', 'prescription', 'memo', 'private']\n",
    "X_train = pd.DataFrame(words_in_texts(some_words, train['email']))\n",
    "Y_train = np.array(train['spam'])\n",
    "X_train[:5], Y_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will try training a Logistic Regression Model with our training set. I find that the accuracy is only about 0.76."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "training_accuracy = model.score(X_train, Y_train)\n",
    "print(\"Training Accuracy: \", training_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating precision and recall:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(X_train)\n",
    "TP = sum((preds == 1) & (Y_train == 1))\n",
    "FP = sum((preds == 1) & (Y_train == 0))\n",
    "TN = sum((preds == 0) & (Y_train == 0))\n",
    "FN = sum((preds == 0) & (Y_train == 1))\n",
    "logistic_predictor_precision = TP/(TP+FP)\n",
    "logistic_predictor_recall = TP/(TP+FN)\n",
    "logistic_predictor_fpr = FP/(FP+TN)\n",
    "\n",
    "print(f\"{TP=}, {TN=}, {FP=}, {FN=}\")\n",
    "print(f\"{logistic_predictor_precision=:.2f}, {logistic_predictor_recall=:.2f}, {logistic_predictor_fpr=:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are more false negatives, there are 1699 false negatives versus 122 false positives. Predicting 0 for every email would give us 0.7447091707706642% prediction accuracy. Since this is very close to the predicton accuracy of our logistic regression classifier, this suggests that the prediction accuracy for our logistic regression classifier was not very good. This tells us that we might as well as marked no emails as spam and gotten nearly the same accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building my own model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_words = ['>', '<p', '!!!', 'offer', 'business', 'please', '2002', 'border=\"0\"', 'but', 'they'] #'savings'\n",
    "X_train = words_in_texts(some_words, train['email'])\n",
    "Y_train = np.array(train['spam'])\n",
    "\n",
    "my_model = LogisticRegression()\n",
    "my_model.fit(X_train, Y_train)\n",
    "my_model.score(X_train, Y_train)\n",
    "\n",
    "train_c = train.copy()\n",
    "val_c = val.copy()\n",
    "def contains_col(dataframe, wordlist):\n",
    "    for i in wordlist:\n",
    "        dataframe[i] = dataframe['email'].apply(lambda x: 1 if i in x else 0)\n",
    "contains_col(train_c, some_words)\n",
    "contains_col(val_c, some_words)\n",
    "train_c[\"! count\"] = train_c['email'].str.count('!')\n",
    "val_c[\"! count\"] = val_c['email'].str.count('!')\n",
    "trainCX = train_c.iloc[:, 4:]\n",
    "trainCY = train['spam']\n",
    "valCX = val_c.iloc[:, 4:]\n",
    "valCY = val_c['spam']\n",
    "model3 = LogisticRegression()\n",
    "model3.fit(trainCX, trainCY)\n",
    "preds = model3.predict(valCX)\n",
    "TP = sum((preds == 1) & (val_c['spam'] == 1))\n",
    "FP = sum((preds == 1) & (val_c['spam'] == 0))\n",
    "TN = sum((preds == 0) & (val_c['spam'] == 0))\n",
    "FN = sum((preds == 0) & (val_c['spam'] == 1))\n",
    "logistic_predictor_precision = TP/(TP+FP) ##low means many false positives \n",
    "logistic_predictor_recall = TP/(TP+FN) ##low means many false negatives\n",
    "logistic_predictor_fpr = FP/(FP+TN)\n",
    "logistic_predictor_fnr = FN/(FN+TP)\n",
    "\n",
    "print(f\"{TP=}, {TN=}, {FP=}, {FN=}\")\n",
    "print(f\"{logistic_predictor_precision=:.2f}, {logistic_predictor_recall=:.2f}, {logistic_predictor_fpr=:.2f}, {logistic_predictor_fnr=:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results: TP=141, TN=595, FP=18, FN=81,\n",
    "logistic_predictor_precision=0.89, logistic_predictor_recall=0.64, logistic_predictor_fpr=0.03, logistic_predictor_fnr=0.36"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start with, I wanted to find the most impactful words, so I created a table with every word in the emails, a \"spam_counts\" column (counts occurrences of that word in spam emails), a \"ham_counts\" column (counts occurrences of that word in ham emails) and \"spam/ham\" (ratio of spam_counts to ham_counts). I initially chose words that had very large spam/ham ratios and very low spam/ham ratios, but I found that this returned low precision and recall rates. I realized this was probably because some of these words had extremely low spam_counts and ham_counts, and there was colinearity among some of the features. So, I filtered the table to only include words with spam_counts and ham_counts greater than 500, and picked the lowest and highest ratios from that table. I went through a process of trial and error trying to find the words that created the highest accuracy, precision and recall rates until I found a good fit. I also added a column for number of exclamation marks because I found that spam emails had more than 3x as many exclamation marks as ham emails. \n",
    "At first, I simply chose words with the highest \"spam/ham\" ratio, but found that precision and recall rates were very low. So, I filtered the table to only include spam_counts and ham_counts greater than 500. This helped but the precision and recall were still low, especially the recall rates. I realized I needed to also inclue low \"spam/ham\" ratio, and ended up choosing 5 words with a very low \"spam/ham\" ratio, and 5 words with a high ratio. Ending up with these 10 words required me to go through the list multiple times, adding and removing words and seeing how it impacted my precision and recall, until I found a list that seemed like the best fit. The most surprising thing for me when finding good features was that the best features were words I would have never considered, such as \"border=\"0\"\". It's surprising to me that indicators such as these are so common in ham emails. Another thing that was kind of surprising is how certain words that had very large \"spam/ham\" ratios had a negative impact on the model, as I would expect they would help the model train better. In fact, surprisingly the words with the highest ratios were some of the most damaging to the model's accuracy, and the best words were the ones that did not have such a stark difference in the number of occurences in spam vs ham emails. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating heapmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['face=\"verdana\"><font', 'width=3d\"550\"', 'align=3d\"right\"><font', 'align=3d\"center\">=20', 'spamassassin-sightings@lists.sourceforge.net', 'wrote:', 'bgcolor=\"#000000\"><img', 'url:', 'size=\"-1\">', 'height=\"9\"']\n",
    "train_cop = train.copy()\n",
    "def contains_col(dataframe, wordlist):\n",
    "    for i in wordlist:\n",
    "        dataframe[i] = dataframe['email'].apply(lambda x: 1 if i in x else 0)\n",
    "contains_col(train_cop, words)\n",
    "train_cop[\"! count\"] = train_cop['email'].str.count('!')\n",
    "trainCOP = train_cop.iloc[:, 4:]\n",
    "correl = trainCOP.corr()\n",
    "sns.heatmap(correl, annot=True, annot_kws={\"size\":8})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This heapmap was used to test the words I was originally going to use for my model, but those words were unsuitable and the heatmap above offers one reason why that might be the case. There is strong colinearity among many of the features. In fact, 7 out of 10 have colinearity with at least 2 other features. This will strongly negatively impact the accuracy of the model's predictions, which helps explain why my precision and recall rates were so low when using these words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.predict_proba(trainCX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "import plotly.express as px\n",
    "model = model3\n",
    "y = trainCY\n",
    "x = trainCX\n",
    "def predict_threshold(model, X, T): \n",
    "    prob_one = model.predict_proba(X)[:, 1]\n",
    "    return (prob_one >= T).astype(int)\n",
    "\n",
    "def tpr_threshold(X, Y, T):\n",
    "    Y_hat = predict_threshold(model, X, T)\n",
    "    return np.sum((Y_hat == 1) & (Y == 1)) / np.sum(Y == 1)\n",
    "\n",
    "def fpr_threshold(X, Y, T):\n",
    "    Y_hat = predict_threshold(model, X, T)\n",
    "    return np.sum((Y_hat == 1) & (Y == 0)) / np.sum(Y == 0)\n",
    "\n",
    "thresholds = np.linspace(0, 1, 100)\n",
    "tprs = [tpr_threshold(x, y, t) for t in thresholds]\n",
    "fprs = [fpr_threshold(x, y, t) for t in thresholds]\n",
    "\n",
    "fig = px.line(x=fprs, y = tprs, hover_name = thresholds, title=\"ROC Curve\")\n",
    "fig.update_xaxes(title = \"False Positive Rate\")\n",
    "fig.update_yaxes(title = \"True Positive Rate\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
